{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifmV02zKlsCs",
        "outputId": "51676b81-9af2-48ef-d959-9561b0e6f110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/493.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m256.0/493.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "  datasets==2.14.6 \\\n",
        "  transformers==4.35.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4vTJ-pFmWl5"
      },
      "source": [
        "## Dataset Download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFaaDw5VmZEk"
      },
      "source": [
        "We're going to test with a more real world use-case, with messy, imperfect data. We will use the [`jamescalam/ai-arxiv-chunked`](https://huggingface.co/datasets/jamescalam/ai-arxiv-chunked) dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234,
          "referenced_widgets": [
            "46a538f703284945aa02f1a9635df725",
            "bf016271032a4e17afd077c076db7f9e",
            "546c045b173045d3a6168f8026d9908b",
            "6e56ad307c274150a9c5c76cbf09d880",
            "cf1fed38a4404b788223d40213409c0c",
            "005c01f2d70f489a916c599ab70eb6c5",
            "be347519a0ed45ac96dea8ec2bb34c63",
            "635b65dfd6ba47f9b4f164a7172f17b8",
            "1a40e659476e48e592114ecd8240cb66",
            "4bf14e38d71f4dca8f4c86275d37eb70",
            "5cca38628853466da8c71bb903812c1a",
            "2bc47670f8f54849a579d33cad530e26",
            "dccc70fc7f2f4ab9bcc32df626316ea1",
            "365d09df31db49528f2ca5bceea799fa",
            "c8813fa73fc446959d602f81780c5141",
            "0b761b2d78da41ea96395d6ab98aef62",
            "7353a0e507eb4dd2a260bbc91d274be4",
            "7057fc73de034b5b99f589f3848f6a9b",
            "03a0dd81c73d4ff388e58d1b2bcd4be1",
            "e4e4309df47a49e3b91c94d96a716c1b",
            "d0e43a541dd240c282ef4d5b3c3e352d",
            "6418264ad7ce4877b39e622a57066b34",
            "a24dd3ad93e44048ae56ec52c2d9a8f0",
            "8d00ae96b30e4342954cfe76378766e0",
            "32aae38a64234ec7864bfbf77bf0121a",
            "834496548500496e84a1242a23b7480f",
            "e12f645b170f4af687a1f5e911e1b3b6",
            "4fb04fd63db44b9e839e181a0a03b60b",
            "8804e9093a1b472980ff8f15b52d0eb3",
            "be5c20b0edd7470abf5693b8858aaab6",
            "1722ce9f0bce4754b844479703d6d650",
            "d0944be7e2b14a6aa6949d299633b5a5",
            "0f895bc9bea84d8c91b80edc5fe23ca9",
            "1c4583c3657f40849f572f739d96b71f",
            "90ec1e18627541749489ea5a06240075",
            "55649c89feaf4bfd9bf47dedbb0f4f0d",
            "dd8a7348e4654636aeb6b5375bfecbad",
            "baded1d2121d4208942a34fd7039e6e4",
            "babb03c39efd495684811fad92568518",
            "936be222c5494abeb189290123d59f1b",
            "b5536f3d243c41d59cf3a76b37cffafb",
            "a2f76bb719604f6782ce31e8d863cdc7",
            "8b32f520e982458b9c21a76c13e6ec75",
            "e7ddd52d516c42faa537abb42b03c693"
          ]
        },
        "id": "4-FqcdKHmVpa",
        "outputId": "17edd6db-e50c-4f55-98fd-dd1be2f32a92"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46a538f703284945aa02f1a9635df725"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/153M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2bc47670f8f54849a579d33cad530e26"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a24dd3ad93e44048ae56ec52c2d9a8f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c4583c3657f40849f572f739d96b71f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
              "    num_rows: 41584\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"jamescalam/ai-arxiv-chunked\", split=\"train\")\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp5a_bInyfdX"
      },
      "source": [
        "First we define our embedding function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oG6zd1dLw54w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227,
          "referenced_widgets": [
            "0e053b6b80e5406d963ec4a7162e7e7a",
            "8f430e6cfe854252b3581cdf653057aa",
            "a9cf699b663b4c948411cce9432282f1",
            "60eee8e8bf9c4366b35d6e98f896938f",
            "66e63d3c3c0646e789ac0bbf3b632763",
            "9a4968b10a4f4ae49cd1ad3e850602c6",
            "e760dd644edf424ba5aa487cc35b6f5a",
            "da95f546b4e5438aa4c0667aa3127883",
            "71474058acea4232b69eb4b43fb7ee94",
            "6dd788bc1ead4567b3503c836cc7f3b1",
            "295e7d1c2eb942bc83cb93d546c39b2a",
            "a015370ddb14463eac743df122bffeb1",
            "756e546dd24a4972a3d1f60e30f195bb",
            "fb7714a94a36466a8dfd58221d047e68",
            "dbf94fe8a85d478eb8fe3fe3e85a5fec",
            "22aba2b20a834ca88e058c771737b628",
            "38bf8a787f1d4d4b8ed9f2e6aae4854e",
            "6654b7221df646b397368b31201c6d51",
            "6f136b5750b844528647cae2b0e321ed",
            "62cc585b43654d95b0ebb53b7f000b09",
            "bd1b70fad0e64967a7919cd5bb40d420",
            "fdca223321b144378cd5b0363b8161e0",
            "a8700142635e4799bec2843cf8c35d2b",
            "73e7d0c40cb34348aa44f66eb983a263",
            "4d97b110bb674cc183f9220ad8b24576",
            "6f1579ac1ac04142b23989cd398ba692",
            "f70a2516358d44d985688a5cb1d2133c",
            "0be4682e42784fc8bdfc62cbfe28c0c1",
            "bb095869cd764b49a094ce17de78f4ef",
            "466602e4f4fb48a3a8275d7310d52b39",
            "873b9ddf32994770a9576c65215ccad7",
            "0e10ef36748e4da8a5072e8e22af3c1f",
            "ec33eea9570f46d0bab6954ab20a5797",
            "8de67b4f5424479c91dde7edbf4ba08e",
            "c31e3a564ea5448eb7c3f385c2e537a0",
            "5d7c2f73ed064a4097b8e626ad3001b9",
            "6730c3b258004df7bc76625df571dfd1",
            "fa472c31d89b44bca153b1bbab269f88",
            "16c952f563924affa3dd24ef6599143d",
            "6846ba9199ed4807a13a8c4fc617bb88",
            "f7f1ab329d634da19694a1cb29f5be3f",
            "edcce28cf81842be8a24f21b45add2d9",
            "d0b9f308c6264b9b8d62c1af3318c527",
            "c26ff52f54ae4d3faef38d43f8373cfa",
            "a97d8927794b4337aa32e6eb2bdd485b",
            "4b7d87f7a3004ebe8d4bb2f262e66c17",
            "952efa8c729746ffbe214ea3450ea1e1",
            "1ef868a0f93f46839a82db7d806cfcad",
            "60dac08102404fdba7e43a0ef4a69723",
            "08c61969c64148bb8a94100e778e3476",
            "6c701dae87594905ad68222c53e82f19",
            "e0b3447cf66941baa17918d4c8612036",
            "072a9afd01dd4ec59dd7f742dd977249",
            "ef68a22a23a74d47be8060bc667da68e",
            "307ff95f17c5471b977763a7b03d6bcd",
            "de5a526529e74731a91e1e2498163236",
            "43ec2715487a4eac99c9844fb09187d4",
            "c3a598b753b7413e8de70d00f07a5492",
            "5dcae3e7c6d4424f8b22f7373e4cd03b",
            "ac7b0e8cb7bd451b84de781031ac6662",
            "b6010033aa3e4f7f812940b043592d2f",
            "14c09479b5024871aa5a9566a4afbb9a",
            "0da469a318874adda13c7eaa97c465b3",
            "7a65e8d693fc49ad976eb557037ff8f0",
            "6d38a86f3754485d9a8bae9d2c7c1f21",
            "7107cc7805c84e1eb59d455e880d37e1"
          ]
        },
        "outputId": "da6f6226-d7a4-482b-fc8f-4087109b0858"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (\u2026)okenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e053b6b80e5406d963ec4a7162e7e7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (\u2026)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a015370ddb14463eac743df122bffeb1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (\u2026)/main/tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a8700142635e4799bec2843cf8c35d2b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (\u2026)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8de67b4f5424479c91dde7edbf4ba08e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (\u2026)lve/main/config.json:   0%|          | 0.00/650 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a97d8927794b4337aa32e6eb2bdd485b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de5a526529e74731a91e1e2498163236"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "from torch.nn.functional import normalize\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device}\")\n",
        "\n",
        "model_id = \"intfloat/e5-base-v2\"\n",
        "\n",
        "# initialize tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModel.from_pretrained(model_id).to(device)\n",
        "model.eval()\n",
        "\n",
        "def embed(docs: list[str]) -> list[list[float]]:\n",
        "    docs = [f\"passage: {d}\" for d in docs]\n",
        "    # tokenize\n",
        "    tokens = tokenizer(\n",
        "        docs, padding=True, max_length=512, truncation=True, return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "    with torch.no_grad():\n",
        "        # process with model for token-level embeddings\n",
        "        out = model(**tokens)\n",
        "        # mask padding tokens\n",
        "        last_hidden = out.last_hidden_state.masked_fill(\n",
        "            ~tokens[\"attention_mask\"][..., None].bool(), 0.0\n",
        "        )\n",
        "        # create mean pooled embeddings\n",
        "        doc_embeds = last_hidden.sum(dim=1) / \\\n",
        "            tokens[\"attention_mask\"].sum(dim=1)[..., None]\n",
        "    return doc_embeds.cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nvrNQSGXvEC"
      },
      "source": [
        "Use this to build a Numpy array of cohere embedding vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EdyWVR17zX7I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3434b2cb9ed644b095b1e22a6b8e9d20",
            "0804affd385e46d28cc0f03799bfdd1c",
            "7893e4b1fd8f4b3f8f0311c755f00a98",
            "0604c2c210854c649ad171759a572f3c",
            "e1584d310298460f84748bbe3bf78b78",
            "6851163139c8409ab2d352845f13259a",
            "167df21c3aa441cea01f7e2343a0502c",
            "fc964edf31614561901debbeed631c05",
            "b631f2168e334ceba90e16be135d73c3",
            "f8e30dca5fbc4b0ebe0caed6ccb97224",
            "98508c32bed84af093a687556e4488cd"
          ]
        },
        "outputId": "9f98aa41-4a43-43b2-c85b-c39ca2254be5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/163 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3434b2cb9ed644b095b1e22a6b8e9d20"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "\n",
        "chunks = data[\"chunk\"]\n",
        "batch_size = 256\n",
        "\n",
        "for i in tqdm(range(0, len(chunks), batch_size)):\n",
        "    i_end = min(len(chunks), i+batch_size)\n",
        "    chunk_batch = chunks[i:i_end]\n",
        "    # embed current batch\n",
        "    embed_batch = embed(chunk_batch)\n",
        "    # add to existing np array if exists (otherwise create)\n",
        "    if i == 0:\n",
        "        arr = embed_batch.copy()\n",
        "    else:\n",
        "        arr = np.concatenate([arr, embed_batch.copy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bl9g3ePt029u"
      },
      "source": [
        "Now we need to create the query mechanism, this is simply a cosine similarity calculation between a query vector and our `arr` vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MR7WyDiatlsX"
      },
      "outputs": [],
      "source": [
        "from numpy.linalg import norm\n",
        "\n",
        "# convert chunks list to array for easy indexing\n",
        "chunk_arr = np.array(chunks)\n",
        "\n",
        "def query(text: str, top_k: int=3) -> list[str]:\n",
        "    # create query embedding\n",
        "    xq = embed([f\"query: {text}\"])[0]\n",
        "    # calculate cosine similarities\n",
        "    sim = np.dot(arr, xq.T) / (norm(arr, axis=1)*norm(xq.T))\n",
        "    # get indices of top_k records\n",
        "    idx = np.argpartition(sim, -top_k)[-top_k:]\n",
        "    docs = chunk_arr[idx]\n",
        "    for d in docs.tolist():\n",
        "        print(d)\n",
        "        print(\"----------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2NjYxsn7J5f",
        "outputId": "6a3f6178-165e-4721-af4b-3e8e4fcd47b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rflow_answerability_classificationtask957_e2e_data_to_texttask288_gigaword_title_generationtask1728_web_nlg_data_to_texttask1358_xlsum_title_generationtask1529_scitailv1.1_textual_entailmenttask349_squad2.0_answerability_classificationtask1442_doqa_answerability_classificationtask640_e_snli_textual_\n",
            "----------\n",
            "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
            "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
            "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
            "Sergey Edunov Thomas Scialom\u0003\n",
            "GenAI, Meta\n",
            "Abstract\n",
            "In this work, we develop and release Llama 2, a collection of pretrained and \ufb01ne-tuned\n",
            "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
            "Our \ufb01ne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\n",
            "models outperform open-source chat models on most benchmarks we tested, and based on\n",
            "ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to \ufb01ne-tuning and safety\n",
            "----------\n",
            "asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavily\ufb01ne-tunedtoalignwithhuman\n",
            "preferences, which greatly enhances their usability and safety. This step can require signi\ufb01cant costs in\n",
            "computeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\n",
            "the community to advance AI alignment research.\n",
            "In this work, we develop and release Llama 2, a family of pretrained and \ufb01ne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\n",
            "L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\n",
            "L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\n",
            "be on par with some of the closed-source models, at least on the human evaluations we performed (see\n",
            "----------\n"
          ]
        }
      ],
      "source": [
        "query(\"why should I use llama 2?\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "twqkaMHqG6E_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZSIBTMUy7qc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceedbca8-e462-49c0-c736-0253aa2725ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\u2212Z0\u22129]([0\u22129]{2}?)\n",
            "Some of the PII appears to be neither real nor accurate, and instead was \"hallucinated\" by the AI assistant.\n",
            "For example, in Figure 12 the address provided does not correspond to a real, physical location and has no\n",
            "public links to the individual named. However, in an abundance of caution, we redacted the name and street\n",
            "address. As described in \u00a7A.7, we removed all PII matches caught by the regex \ufb01lter before publicly releasing\n",
            "the dataset.\n",
            "A.7 Datasheet\n",
            "Motivation\n",
            "For what purpose was the dataset created? Was there a speci\ufb01c task in mind? Was there a speci\ufb01c gap that needed to be \ufb01lled? Please provide a description.\n",
            "\u2022 We created this dataset to analyze and address potential harms in large language models through a\n",
            "process of adversarial testing known as \u201cred teaming\u201d. We publicly release the dataset for further\n",
            "analysis and exploration by the research community. This dataset adds to a limited number of\n",
            "publicly-available red team datasets, and to our knowledge it is the only dataset of red team attacks\n",
            "on a language model trained with reinforcement learning from human feedback (RLHF) as a safety\n",
            "technique.\n",
            "----------\n",
            "for red teaming (\u00a73). Throughout the design of our experiments, we arrived at many junctures in which\n",
            "we were unsure about how to proceed, even after a literature review on red teaming AI systems (\u00a72). As\n",
            "such, we conducted informational interviews with experts in the \ufb01eld of Trust & Safety and incorporated\n",
            "their suggested best practices (\u00a7A.2) into the design of our experiments in order to ensure the well-being of\n",
            "the red team. In general, we found that red team members enjoyed participating in our experiments and felt\n",
            "motivated by a mission to make AI systems less harmful (\u00a7A.2). Nevertheless, our work suffers from some\n",
            "limitations, which we discuss in \u00a75.1. Based on our experiences, we propose some policy interventions for\n",
            "how we can work together as a community to develop shared norms, practices, and technical standards for\n",
            "how to red team language models (\u00a75.2).\n",
            "2 Related Work\n",
            "We use the same models that we developed in our previous work where we train a general language assistant\n",
            "to be helpful, honest, and harmless [2, 4]. However, here we run additional experiments in order to determine\n",
            "the in\ufb02uence of model size on susceptibility to red team attacks (Figure 1) and analyze the content of the\n",
            "----------\n",
            "\u679c\uff0c\u5e76\u89e3\u91ca\u6211\u6240\u6301\u7acb\u573a\u7684\u539f\u56e0\u3002\u56e0\u6b64\uff0c\u6211\u81f4\u529b\u4e8e\u63d0\u4f9b\u79ef\u6781\u3001\u6709\u8da3\u3001\u5b9e\u7528\u548c\u5438\u5f15\u4eba\u7684\u56de\n",
            "\u7b54\u3002\u6211\u7684\u903b\u8f91\u548c\u63a8\u7406\u529b\u6c42\u4e25\u5bc6\u3001\u667a\u80fd\u548c\u6709\u7406\u6709\u636e\u3002\u53e6\u5916\uff0c\u6211\u53ef\u4ee5\u63d0\u4f9b\u66f4\u591a\u76f8\u5173\u7ec6\u8282\u6765\n",
            "\u5168\u9762\u6df1\u5165\u5730\u56de\u7b54\u95ee\u9898\uff0c\u6db5\u76d6\u8bdd\u9898\u7684\u5404\u4e2a\u65b9\u9762\u3002\u8bf7\u968f\u65f6\u5411\u6211\u63d0\u95ee\uff0c\u6211\u4f1a\u975e\u5e38\u9ad8\u5174\u4e3a\u60a8\u63d0\n",
            "\u4f9b\u5e2e\u52a9\u3002\n",
            "### User\n",
            "[User Query]\n",
            "### Dromedary\n",
            "[Dromedary Response]\n",
            "39\n",
            "F 20 Seed Prompts for Topic-Guided Red-Teaming Self-Instruct\n",
            "----------\n"
          ]
        }
      ],
      "source": [
        "query(\"can you tell me about red teaming for llama 2?\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query(\"what is the best llm?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VUqwF-com3z",
        "outputId": "e7951ad7-a2f9-4c6f-f395-05fa7654ad1f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "by 3.37 and 1.53, respectively. This result exhibits\n",
            "the potential of distilling LLMs for IR, as the distilled models are trained using only 10K samples,\n",
            "costing approximately $40 for API calls, and without human annotation. This approach is much more\n",
            "cost-ef\ufb01cient than previous systems that have been\n",
            "trained on over 400K annotated data.\n",
            "5 Conclusion\n",
            "In this paper, we conduct a comprehensive study\n",
            "on passage re-ranking with instruction-following\n",
            "LLMs. Besides the institutional query generation\n",
            "and relevance generation methods, we introduce\n",
            "a novel permutation generation approach to fully\n",
            "explore the power of LLMs. Our experiments on\n",
            "three benchmarks have demonstrated the capability of ChatGPT and GPT-4 in passage re-ranking.\n",
            "Furthermore, we propose a permutation distillation\n",
            "method and show its advantages over existing supervised approaches in terms of effectiveness and\n",
            "ef\ufb01ciency.\n",
            "For future work, we suggest some promising directions on LLMs for IR: (1) LLMs as relevance\n",
            "annotators. Data labeling is quite expensive in IR,\n",
            "and as our pilot experiments have demonstrated\n",
            "the effectiveness of the relevance judgments generated by LLMs, we believe it deserves further\n",
            "exploration. (2) Instruction-tuning LLMs for a\n",
            "----------\n",
            "know which model they come from.\n",
            "PLEASE READ THESE INSTRUCTIONS IN FULL.\n",
            "Annotation Rules:\n",
            "* Rank the responses according to which one provides the best\n",
            "answer to the input prompt.\n",
            "* What is the best answer? Make a decision based on (a) the\n",
            "correctness of the answer, and (b) the informativeness of the\n",
            "response. For (a) you are allowed to search the web. Overall,\n",
            "use your best judgment to rank answers based on being the most\n",
            "useful response, which we define as one which is at least somewhat correct,\n",
            "and minimally informative about what the prompt is asking for.\n",
            "* If two responses provide the same correctness and informativeness\n",
            "by your judgment, and there is no clear winner, you may rank them the\n",
            "same, but please only use this sparingly.\n",
            "* If the answer for a given response is nonsensical, irrelevant,\n",
            "highly ungrammatical/confusing, or does not clearly respond to the\n",
            "given prompt, label it with \u201cF\u201d (for fail) rather than its rank.\n",
            "* Long answers are not always the best. Answers which provide\n",
            "succinct, coherent responses may be better than longer ones, if they\n",
            "are at least as correct and informative.\n",
            "53\n",
            "J Author Contributions\n",
            "----------\n",
            "the LLMs\u2019 ability for searching. For example, New\n",
            "Bing utilizes GPT-4 to generate responses based on\n",
            "the retrieved documents (Microsoft, 2023). As a\n",
            "Figure 1: Average results of ChatGPT and GPT-4\n",
            "(zero-shot) on passage re-ranking benchmarks (TREC,\n",
            "BEIR, and Mr.TyDi), compared with BM25 and\n",
            "previous best supervised systems (SOTA sup., e.g.,\n",
            "monoT5 (Nogueira et al., 2020)).\n",
            "result, it is still unclear whether LLMs, e.g., ChatGPT, are good at search.\n",
            "To this end, this paper aims to investigate the potential of LLMs in relevance ranking for IR. Specifically, we focus on the following two questions:\n",
            "\u2022(RQ1) How does ChatGPT perform on passage re-ranking tasks?\n",
            "\u2022(RQ2) How to imitate the ranking capabilities\n",
            "of ChatGPT to a smaller, specialized model?\n",
            "To answer the \ufb01rst question, we explore two\n",
            "strategies (Sachan et al., 2022a; Liang et al., 2022)\n",
            "to instruct ChatGPT performing on passage reranking tasks, which we named instructional query\n",
            "generation andinstructional relevance generation .\n",
            "However, we observe that these methods have limited performance in re-ranking and heavily rely\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query(\"what is the difference between gpt-4 and llama 2?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sF8xvtyaoom4",
        "outputId": "5f9579b4-2154-473b-fa1a-dcf0a9ab8305"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.043\n",
            "-0.009+0.0132-0.004 +0.0562\n",
            "+0.0387-0.012\n",
            "-0.076Alpaca: 0.39 LLaMA-GPT4: 0.34 GPT4: 0.37Figure 6: ROUGE-L on unnatural instructions evaluated with 9K samples. The instructions are\n",
            "grouped into four subsets based on the ground-truth response length. The mean values are reported in\n",
            "the legend. The difference with GPT-4 is reported on the bar per group. LLaMA-GPT4 is a closer\n",
            "proxy to GPT-4 than Alpaca.\n",
            "closely follow the behavior of GPT-4. When the sequence length is short, both LLaMA-GPT4 and\n",
            "GPT-4 can generate responses that contains the simple ground truth answers, but add extra words to\n",
            "make the response more chat-like, which probably leads to lower ROUGE-L scores.\n",
            "5 R ELATED WORK\n",
            "Instruction Tuning. Instruction tuning of LLMs is an increasingly popular research direction in\n",
            "NLP (Zhong et al., 2021; Ouyang et al., 2022; Wei et al., 2021). Existing works aim to improve\n",
            "the quality and scale of three factors in the development pipeline, including instruction-following\n",
            "----------\n",
            "(ii)For GPT-4 results alone, the translated responses show superior performance over the generated\n",
            "response in Chinese, probably because GPT-4 is trained in richer English corpus than Chinese, which\n",
            "leads to stronger English instruction-following ability. In Figure 5 (c), we show results for all models\n",
            "who are asked to answer in Chinese.\n",
            "We compare LLaMA-GPT4 with GPT-4 and Alpaca unnatural instructions in Figure 6. In terms of the\n",
            "average ROUGE-L scores, Alpaca outperforms the other two models. We note that LLaMA-GPT4 and\n",
            "GPT4 is gradually performing better when the ground truth response length is increasing, eventually\n",
            "showing higher performance when the length is longer than 4. This means that they can better follow\n",
            "instructions when the scenarios are more creative. Across different subsets, LLaMA-GPT4 can\n",
            "7\n",
            "0-2 3-5 6-10 10>\n",
            "Groundtruth Response Length0.30.40.5RougeL\n",
            "-0.043\n",
            "-0.009+0.0132-0.004 +0.0562\n",
            "+0.0387-0.012\n",
            "----------\n",
            "31.39%LLaMA-GPT4 \n",
            " 25.99%\n",
            "Tie \n",
            " 42.61%\n",
            "HonestyAlpaca \n",
            "  25.43%LLaMA-GPT4 \n",
            " 16.48%\n",
            "Tie \n",
            " 58.10%\n",
            "Harmlessness(a) LLaMA-GPT4 vs Alpaca ( i.e.,LLaMA-GPT3 )\n",
            " GPT4 \n",
            "  44.11%\n",
            "LLaMA-GPT4 \n",
            " 42.78% Tie \n",
            " 13.11%\n",
            "Helpfulness GPT4 \n",
            "  37.48%\n",
            "LLaMA-GPT4 \n",
            " 37.88% Tie \n",
            " 24.64%\n",
            "Honesty GPT4 \n",
            "  35.36% LLaMA-GPT4 \n",
            " 31.66% Tie \n",
            " 32.98%\n",
            "Harmlessness\n",
            "(b) LLaMA-GPT4 vs GPT-4\n",
            "Figure 3: Human evaluation.\n",
            "4.2 H UMAN EVALUATION WITH ALIGNMENT CRITERIA\n",
            "To evaluate the alignment quality of our instruction-tuned LLMs, we follow alignment criteria from\n",
            "Anthropic Askell et al. (2021): an assistant is aligned if it is helpful, honest, and harmless (HHH).\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "d2UP-QJzo610"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}